{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2ee89896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale \n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bead6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[5 0 4 ... 5 6 8]\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "(10000, 28, 28)\n",
      "[7 2 1 ... 4 5 6]\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "file_directory='../Preprocess/data_after/'\n",
    "#opposite과 my_team에 대한 변수를 두어 그 값이 1이면 내 팀, 0이면 다른 팀 인 방식으로 구현\n",
    "#테스트 데이터는 match.csv의 result attribute를 사용\n",
    "#train_data에 사용되는 데이터는 player1.csv의 선수의 티어 값을 이용\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(X_test)\n",
    "print(X_test.shape)\n",
    "print(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3529abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CJ Intas', 'DRK', 'DWG KOE', 'Efriice Friics', 'Fridot BROUN', 'Gin.G', 'Groffon', 'Henwhe Lofi Ispurts', 'ISC Ivir', 'Ivir8 Wonnirs', 'Jon Eor Griin Wongs', 'KSV iSpurts', 'KT Rulstir', 'Kongzuni DregunX', 'Kungduu Munstir', 'Loov SENDBUX', 'Lungzha Gemong', 'MBP', 'NeJon i-mFori', 'Nungshom RidFurci', 'Oncridobli Morecli', 'RUX Togirs', 'Ribils Enerchy', 'SBINA Kurie', 'SK Tilicum T1', 'Semsang Gelexy', 'SiulHeiUni Pronci', 'T2', 'Tiem Dynemocs', 'bbq Ulovirs', 'i-mFori']\n",
      "[[1.78883134]\n",
      " [4.60952047]\n",
      " [2.2977072 ]\n",
      " [1.17830484]\n",
      " [1.77295721]\n",
      " [1.64449551]\n",
      " [4.62373209]\n",
      " [1.51260863]\n",
      " [2.14557571]\n",
      " [0.38450405]\n",
      " [3.34106165]\n",
      " [1.6589373 ]\n",
      " [3.2411491 ]\n",
      " [3.94795031]\n",
      " [3.93775704]\n",
      " [3.50304002]\n",
      " [4.92025396]\n",
      " [0.11374055]\n",
      " [1.09701196]\n",
      " [2.08894976]\n",
      " [1.15857176]\n",
      " [1.73497676]\n",
      " [4.61568436]\n",
      " [3.93105205]\n",
      " [2.93329524]\n",
      " [4.50163785]\n",
      " [2.04833752]\n",
      " [4.41792697]\n",
      " [2.63842029]\n",
      " [2.14418288]\n",
      " [4.06636711]]\n",
      "[[2.57142857 1.78883134]\n",
      " [1.88235294 4.60952047]\n",
      " [1.94117647 2.2977072 ]\n",
      " [2.51282051 1.17830484]\n",
      " [3.11111111 1.77295721]\n",
      " [2.47727273 1.64449551]\n",
      " [2.12       4.62373209]\n",
      " [2.76       1.51260863]\n",
      " [3.         2.14557571]\n",
      " [3.875      0.38450405]\n",
      " [3.08974359 3.34106165]\n",
      " [3.16666667 1.6589373 ]\n",
      " [2.46067416 3.2411491 ]\n",
      " [1.92       3.94795031]\n",
      " [3.55555556 3.93775704]\n",
      " [2.75675676 3.50304002]\n",
      " [2.44827586 4.92025396]\n",
      " [3.06666667 0.11374055]\n",
      " [2.5625     1.09701196]\n",
      " [2.6        2.08894976]\n",
      " [3.35294118 1.15857176]\n",
      " [2.46666667 1.73497676]\n",
      " [3.         4.61568436]\n",
      " [3.73333333 3.93105205]\n",
      " [2.1369863  2.93329524]\n",
      " [2.55       4.50163785]\n",
      " [3.47058824 2.04833752]\n",
      " [1.96       4.41792697]\n",
      " [2.8        2.63842029]\n",
      " [3.125      2.14418288]\n",
      " [3.83333333 4.06636711]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#player1.csv의 파일을 읽어서 각 팀별 플레이어 티어 평균값을 추출\n",
    "data=pd.read_csv(\"{0}player1.csv\".format(file_directory))\n",
    "tier = data.loc[:,['Team','Tier']]\n",
    "\n",
    "team = data.sort_values('Team' ,ascending=True)\n",
    "team = team.loc[:,'Team'].drop_duplicates()\n",
    "team = team.to_list()\n",
    "print(team)\n",
    "AvgTier = tier.groupby('Team').mean()\n",
    "AvgTier2 = np.random.rand(AvgTier.shape[0])\n",
    "AvgTier2 = AvgTier2.reshape((31,1))*5\n",
    "print(AvgTier2)\n",
    "\n",
    "\n",
    "AvgTier = AvgTier.to_numpy()\n",
    "\n",
    "Avg = np.zeros((31,2))\n",
    "\n",
    "AvgTier = AvgTier.reshape(31)\n",
    "AvgTier2 = AvgTier2.reshape(31)\n",
    "\n",
    "Avg[:,0] = AvgTier\n",
    "Avg[:,1] = AvgTier2\n",
    "\n",
    "\n",
    "Answer = np.zeros(31)\n",
    "\n",
    "\n",
    "for i in range(AvgTier.shape[0]):\n",
    "    if AvgTier[i]<AvgTier2[i]:\n",
    "        Answer[i] = 1\n",
    "    else:\n",
    "        Answer[i] = 0\n",
    "\n",
    "print(Avg)\n",
    "Answer = Answer.reshape((31,1))\n",
    "print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73915129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26410, 3)\n"
     ]
    }
   ],
   "source": [
    "#match.csv 파일을 읽어서 팀의 승패 결과를 test 데이터로 활용\n",
    "data=pd.read_csv(\"{0}match.csv\".format(file_directory))\n",
    "X_test = data.loc[:,['side','team','result']]\n",
    "X_test = X_test.to_numpy()\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "45ce2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26410, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ab05044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_samples = X_train.shape[0] \n",
    "width = X_train.shape[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c976fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26410\n"
     ]
    }
   ],
   "source": [
    "num_of_test_samples = X_test.shape[0] \n",
    "print(num_of_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82f08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3aeb970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9405adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer\n",
    "model.add(Dense(256, input_dim=width, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bad63f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer 1\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "14a6273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer 2\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu')) \n",
    "model.add(Dropout(0.3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d0bbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer 3\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu')) \n",
    "model.add(Dropout(0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "46751457",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_class = 10 \n",
    "model.add(Dense(number_of_class, activation='relu'))#soft_max \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a71e003f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 31\n  y sizes: 60000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-d8d78b319d29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtraining_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 31\n  y sizes: 60000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "training_epochs = 15 \n",
    "batch_size = 100 \n",
    "model.fit(X_train, y_train, epochs=training_epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d927c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 950us/step - loss: 0.0749 - accuracy: 0.9809\n",
      "Accuracy: 0.98089998960495\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_test, batch_size=batch_size) \n",
    "print('Accuracy: ' + str(evaluation[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
